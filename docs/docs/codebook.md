# Data Dictionary 
**Design alignment:** 4 cultures × 3 models × 4 functions × 4 scenarios × 2 complexity × 2 languages  
**Units:** seconds for time; minutes for Evaluation_Time; 1–5 Likert for scores

## TaskID Format

**Pattern:** `[Culture]-[Function]-[Complexity]-[Scenario]-Index`  
**Example:** `CN_TCU_H_BUS_001_GPT41`

## Controlled Vocabularies

| Category | Values | Code (Recommended) |
|----------|--------|-------------------|
| **Culture** | Chinese \| American \| German \| Swedish | CN \| US \| DE \| SE |
| **Function** | TCU \| EEI \| SNA \| VCC | - |
| **Scenario** | BUS \| EDU \| SOC \| FAM | - |
| **Complexity** | Low \| High | - |
| **Language** | EN \| Native | - |
| **Evaluator_ID** | A \| B \| C \| D | - |
| **Flags** (multi-select) | stereotype \| essentialism \| hallucination \| unsafe \| refusal \| off-topic \| other | - |

## Core Fields (Required)

| Field Name | Meaning | Type/Allowed Values | Notes |
|------------|---------|-------------------|-------|
| **TaskID** | Unique task identifier | string | e.g., `CN_TCU_H_BUS_001_GPT41` |
| **Timestamp** | Timestamp of model response | string/datetime | ISO 8601 (e.g., `2025-09-08T12:34:56Z`) |
| **Model** | Model name as logged | string | e.g., `GPT-4.1` / `Claude-4` / `DeepSeek-V3` / `Gemini-2.5` |
| **Model_Version** | Exact model version/build | string | Keep API-reported version/hash if available |
| **Culture** | Target culture | string | Chinese / American / German / Swedish |
| **Function** | Functional dimension | string | TCU / EEI / SNA / VCC |
| **Complexity** | Task complexity | string | Low / High |
| **Language** | Language condition | string | EN / Native |
| **Scenario** | Scenario category | string | BUS / EDU / SOC / FAM |
| **Prompt_Text** | Full prompt | string | If sensitive, anonymize per data policy |
| **Response_Text** | Full model response | string | - |
| **Response_Time** | Model generation latency | float | seconds |
| **Word_Count** | Word count of response | integer | Count after normalization (define tokenizer if needed) |
| **Cultural_Keywords** | Detected cultural keywords | string/list | Store as JSON array or comma-separated string |
| **Score_Accuracy** | Conceptual/factual accuracy | integer (1–5) | Likert, see scoring manual |
| **Score_Appropriateness** | Pragmatic appropriateness | integer (1–5) | Likert |
| **Score_Sensitivity** | Cultural sensitivity | integer (1–5) | Likert |
| **Score_Depth** | Contextual depth/mechanism | integer (1–5) | Likert |
| **Score_Overall** | Holistic score | integer (1–5) | Round(mean 4 dims) ±1 with justification |
| **Evaluation_Time** | Rating time spent | float | minutes |
| **Evaluator_ID** | Rater ID | string | A/B/C/D |
| **Confidence_Level** | Rater confidence | integer (1–5) | Likert |
| **Cultural_Validation** | Marked for validator review | string (Y/N) | Y if sampled for validation |
| **Validation_Score** | Validator's overall score | integer (1–5) | If applicable |
| **ICC_Contribution** | Included in ICC calc | string (Y/N) | For overlapping items |
| **source_file** | Source filename | string | Auto-filled during merge |

## Field Descriptions

### Task Identification
- **TaskID**: Unique identifier following the pattern `[Culture]-[Function]-[Complexity]-[Scenario]-Index`
- **Timestamp**: ISO 8601 formatted timestamp of when the model response was generated
- **source_file**: Automatically populated during data merge operations

### Model Information
- **Model**: The specific LLM model used (e.g., GPT-4.1, Claude-4, DeepSeek-V3, Gemini-2.5)
- **Model_Version**: Exact version/build information as reported by the API

### Task Context
- **Culture**: Target culture for the cross-cultural evaluation
- **Function**: Functional dimension being tested (TCU, EEI, SNA, VCC)
- **Complexity**: Task complexity level (Low/High)
- **Language**: Language condition (EN for English, Native for native language)
- **Scenario**: Scenario category (BUS for business, EDU for education, SOC for social, FAM for family)

### Content
- **Prompt_Text**: The complete prompt given to the model
- **Response_Text**: The complete response generated by the model
- **Response_Time**: Time taken for model to generate response (in seconds)
- **Word_Count**: Number of words in the response after normalization
- **Cultural_Keywords**: Keywords detected that relate to cultural concepts

### Scoring
- **Score_Accuracy**: 1-5 Likert scale for conceptual/factual accuracy
- **Score_Appropriateness**: 1-5 Likert scale for pragmatic appropriateness
- **Score_Sensitivity**: 1-5 Likert scale for cultural sensitivity
- **Score_Depth**: 1-5 Likert scale for contextual depth and mechanism
- **Score_Overall**: 1-5 Likert scale for holistic assessment (rounded mean of 4 dimensions ±1 with justification)

### Evaluation Process
- **Evaluation_Time**: Time spent by evaluator rating the response (in minutes)
- **Evaluator_ID**: Identifier for the human evaluator (A, B, C, or D)
- **Confidence_Level**: Evaluator's confidence in their rating (1-5 Likert scale)

### Quality Control
- **Cultural_Validation**: Flag indicating if item was selected for cultural validator review
- **Validation_Score**: Overall score provided by cultural validator (if applicable)
- **ICC_Contribution**: Flag indicating if item is included in inter-rater reliability calculations

## Data Types and Constraints

### Numeric Fields
- **Response_Time**: Float, measured in seconds
- **Evaluation_Time**: Float, measured in minutes
- **Word_Count**: Integer, non-negative
- **All Scores**: Integer, constrained to 1-5 range
- **Confidence_Level**: Integer, constrained to 1-5 range

### String Fields
- **TaskID**: Must follow specified pattern
- **Timestamp**: Must be valid ISO 8601 format
- **Culture**: Must be one of the controlled vocabulary values
- **Function**: Must be one of the controlled vocabulary values
- **Complexity**: Must be "Low" or "High"
- **Language**: Must be "EN" or "Native"
- **Scenario**: Must be one of the controlled vocabulary values
- **Evaluator_ID**: Must be one of: A, B, C, D
- **Cultural_Validation**: Must be "Y" or "N"
- **ICC_Contribution**: Must be "Y" or "N"

### List Fields
- **Cultural_Keywords**: Can be stored as JSON array or comma-separated string
- **Flags**: Multi-select from controlled vocabulary (stereotype, essentialism, hallucination, unsafe, refusal, off-topic, other)

## Usage Notes

1. **TaskID Generation**: Follow the exact pattern for consistency across datasets
2. **Timestamp Format**: Use ISO 8601 for international compatibility
3. **Score Validation**: Ensure all scores are within 1-5 range
4. **Cultural Keywords**: Use consistent extraction methodology
5. **Quality Flags**: Apply flags consistently based on evaluation criteria
6. **Data Privacy**: Anonymize sensitive prompt content per data policy requirements

---

*This data dictionary provides the complete schema for cross-cultural LLM evaluation datasets, ensuring consistent data collection and analysis across all experimental conditions.*
