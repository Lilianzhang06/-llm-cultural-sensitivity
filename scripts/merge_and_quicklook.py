# -*- coding: utf-8 -*-
"""
Merge JSON â†’ CSV â†’ Quick Look figs + (optional) interactive dashboard + markdown report.

Usage:
  python scripts/merge_and_quicklook.py \
      --raw-dir "data/raw" \
      --out "reports" \
      --csv "data/processed/merged_cross_cultural_data.csv" \
      --with-plotly  


  python scripts/merge_and_quicklook.py --raw-dir "data raw" --out "reports"
"""
import os, re, glob, json, argparse, warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pandas import json_normalize

PLOTLY_AVAILABLE = True
try:
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
except Exception:
    PLOTLY_AVAILABLE = False

plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")


def guess_culture_model(fname):
    base = os.path.basename(fname).lower()
    # culture
    if 'china' in base or re.search(r'\bcn\b', base): culture = "Chinese"
    elif 'german' in base or 'germany' in base or re.search(r'\bde(_|\b)', base): culture = "German"
    elif 'swed' in base or 'sweden' in base or 'swedish' in base or 'se_' in base: culture = "Swedish"
    elif re.search(r'\bus\b', base) or 'usa' in base or 'american' in base: culture = "American"
    else: culture = "Unknown"
    # model 
    if re.search(r'claude[-_ ]?4|sonnet\s*4', base): model = "Claude-4"
    elif re.search(r'gpt[-_ ]?4(\.1)?|gpt4', base): model = "GPT-4"
    elif re.search(r'deepseek[-_ ]?v?3', base): model = "DeepSeek-V3"
    elif re.search(r'gemini[-_ ]?2\.5', base): model = "Gemini-2.5"
    else: model = "Unknown"
    return culture, model

def standardize_model(name):
    if not isinstance(name, str): return name
    low = name.lower()
    if 'claude' in low and '4' in low: return 'Claude-4'
    if 'gpt' in low and '4' in low: return 'GPT-4'
    if 'deepseek' in low and ('v3' in low or 'v-3' in low): return 'DeepSeek-V3'
    if 'gemini' in low and ('2.5' in low or '2_5' in low): return 'Gemini-2.5'
    return name


# ---------- è¯»å– JSONï¼Œä¼˜å…ˆè§£æ {task_results: [{basic_info, interaction_data}]} ç»“æ„ ----------
def load_json_records(path):
    """
    è¿”å› DataFrameã€‚è‹¥æ£€æµ‹åˆ° task_results ç»“æ„ï¼Œåˆ™æŠ½å–æ ¸å¿ƒå­—æ®µï¼›
    å¦åˆ™ç”¨ json_normalize é€€åŒ–è§£æï¼Œå¹¶è¡¥å……ä»æ–‡ä»¶åæ¨æ–­çš„ culture/modelã€‚
    """
    # è¯»å–æ–‡æœ¬
    with open(path, 'r', encoding='utf-8') as f:
        txt = f.read().strip()

    # å°è¯•æ•´ä½“json
    try:
        obj = json.loads(txt)
    except json.JSONDecodeError:
        # æŒ‰è¡Œè§£æï¼ˆjsonlinesï¼‰
        records = []
        for line in txt.splitlines():
            line = line.strip()
            if not line: continue
            try:
                records.append(json.loads(line))
            except Exception:
                pass
        if not records:
            return pd.DataFrame()
        df = json_normalize(records, max_level=2)
        return df

    # å¦‚æœæ˜¯æˆ‘ä»¬æœŸæœ›çš„ç»“æ„ï¼š{"task_results": [ {basic_info, interaction_data}, ... ]}
    if isinstance(obj, dict) and isinstance(obj.get('task_results'), list):
        rows = []
        for task in obj['task_results']:
            basic = task.get('basic_info', {}) or {}
            inter  = task.get('interaction_data', {}) or {}
            # Cultural_Keywords æ—¢å¯èƒ½æ˜¯listä¹Ÿå¯èƒ½æ˜¯str
            ck = inter.get('Cultural_Keywords', None)
            if isinstance(ck, (list, tuple)):
                ck = ', '.join([str(x) for x in ck])
            # è®°å½•
            rows.append({
                'task_id': basic.get('TaskID', ''),
                'timestamp': basic.get('Timestamp', ''),
                'model': basic.get('Model', ''),
                'temperature': basic.get('Temperature', ''),
                'culture': basic.get('Culture', ''),
                'function': basic.get('Function', ''),
                'complexity': basic.get('Complexity', ''),
                'language': basic.get('Language', ''),
                'scenario': basic.get('Scenario', ''),
                'prompt_text': inter.get('Prompt_Text', ''),
                'response_text': inter.get('Response_Text', ''),
                'response_time': inter.get('Response_Time', None),
                'word_count': inter.get('Word_Count', None),
                'cultural_keywords': ck,
                'model_version': inter.get('Model_Version', ''),
            })
        return pd.DataFrame(rows)

    # å…¶ä»–ç»“æ„ï¼šlist æˆ– dictï¼Œå°½é‡å±•å¼€
    if isinstance(obj, list):
        return json_normalize(obj, max_level=2)
    if isinstance(obj, dict):
        # å¸¸è§å®¹å™¨ key
        for k in ['results', 'data', 'items', 'records']:
            if isinstance(obj.get(k), list):
                return json_normalize(obj[k], max_level=2)
        return json_normalize(obj, max_level=2)

    return pd.DataFrame()


# ---------- åˆå¹¶ç›®å½•ä¸‹æ‰€æœ‰ JSON ----------
def merge_raw_json(raw_dir):
    files = sorted(glob.glob(os.path.join(raw_dir, "*.json")))
    if not files:
        raise FileNotFoundError(f"æœªåœ¨ {raw_dir} æ‰¾åˆ° .json æ–‡ä»¶")
    frames = []
    total_rows = 0
    for fp in files:
        df = load_json_records(fp)
        if df.empty:
            print(f"è·³è¿‡ï¼šæ— æ³•è§£æ {os.path.basename(fp)}")
            continue
        # è‹¥ç¼ºå°‘ culture/modelï¼Œä»æ–‡ä»¶åæ¨æ–­
        if 'culture' not in df.columns or df['culture'].isna().all():
            c, _ = guess_culture_model(fp)
            df['culture'] = df.get('culture', c) if 'culture' in df.columns else c
        if 'model' not in df.columns or df['model'].isna().all():
            _, m = guess_culture_model(fp)
            df['model'] = df.get('model', m) if 'model' in df.columns else m

        df['source_file'] = os.path.basename(fp)
        frames.append(df)
        total_rows += len(df)
        print(f"è¯»å– {os.path.basename(fp)} â†’ {len(df)} è¡Œ")
    merged = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()
    print(f"åˆå¹¶å®Œæˆï¼š{len(files)} ä¸ªæ–‡ä»¶ï¼Œå…± {total_rows} è¡Œ")
    return merged


# ---------- é¢„å¤„ç†ï¼ˆæ•°å€¼è½¬æ¢ã€æ¨¡å‹åæ ‡å‡†åŒ–ã€æ–‡åŒ–ä»£ç ç­‰ï¼‰ ----------
def preprocess(df):
    # æ•°å€¼åˆ—è½¬æ¢ï¼ˆå°½é‡ä¸ä¸¢è¡Œï¼‰
    for col in ['response_time', 'word_count', 'temperature']:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    # æ ‡å‡†åŒ–æ¨¡å‹å
    if 'model' in df.columns:
        df['model_standardized'] = df['model'].apply(standardize_model)
    else:
        df['model_standardized'] = 'Unknown'

    # æ–‡åŒ–ä»£ç ï¼ˆå¯é€‰ï¼‰
    if 'culture' in df.columns:
        culture_map = {'Chinese':'CN', 'American':'US', 'German':'DE', 'Swedish':'SE'}
        df['culture_code'] = df['culture'].map(culture_map)

    return df


# ---------- å‡ºè¡¨ + å‡ºå›¾ ----------
def export_tables_and_figs(df, out_dir):
    fig_dir = os.path.join(out_dir, "figures")
    tbl_dir = os.path.join(out_dir, "tables")
    os.makedirs(fig_dir, exist_ok=True)
    os.makedirs(tbl_dir, exist_ok=True)

    # 1) ç¼ºå¤±ç‡ & è®¡æ•°è¡¨
    df.isna().mean().sort_values(ascending=False).to_frame("missing_rate").to_csv(
        os.path.join(tbl_dir, "missing_rate.csv"))
    if 'culture' in df.columns and 'model_standardized' in df.columns:
        counts = df.groupby(['culture','model_standardized']).size().reset_index(name='n')
        counts.to_csv(os.path.join(tbl_dir, "counts_by_culture_model.csv"), index=False)

    # 2) åŸºç¡€åˆ†å¸ƒ + ç®±çº¿ï¼ˆè‹¥æœ‰æ•°å€¼ï¼‰
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    # åŸºç¡€åˆ†å¸ƒå›¾ï¼šresponse_time / word_count
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.ravel()

    if 'response_time' in df.columns and df['response_time'].notna().any():
        sns.histplot(df['response_time'].dropna(), bins=30, kde=True, ax=axes[0], color='skyblue')
        axes[0].set_title('å“åº”æ—¶é—´åˆ†å¸ƒ'); axes[0].set_xlabel('ç§’')
    else:
        axes[0].set_visible(False)

    if 'word_count' in df.columns and df['word_count'].notna().any():
        sns.histplot(df['word_count'].dropna(), bins=30, kde=True, ax=axes[1], color='lightgreen')
        axes[1].set_title('è¯æ•°åˆ†å¸ƒ'); axes[1].set_xlabel('è¯æ•°')
    else:
        axes[1].set_visible(False)

    if 'culture' in df.columns and 'response_time' in df.columns and df['response_time'].notna().any():
        sns.boxplot(data=df, x='culture', y='response_time', ax=axes[2], color='#55a868')
        axes[2].set_title('ä¸åŒæ–‡åŒ–çš„å“åº”æ—¶é—´'); axes[2].tick_params(axis='x', rotation=20)
    else:
        axes[2].set_visible(False)

    if 'model_standardized' in df.columns and 'response_time' in df.columns and df['response_time'].notna().any():
        sns.boxplot(data=df, x='model_standardized', y='response_time', ax=axes[3], color='#c44e52')
        axes[3].set_title('ä¸åŒæ¨¡å‹çš„å“åº”æ—¶é—´'); axes[3].tick_params(axis='x', rotation=20)
    else:
        axes[3].set_visible(False)

    plt.tight_layout()
    plt.savefig(os.path.join(fig_dir, "basic_analysis.png"), dpi=300, bbox_inches='tight')
    plt.close()
    print("ä¿å­˜ï¼šfigures/basic_analysis.png")

    # æ–‡åŒ–Ã—æ¨¡å‹çƒ­åŠ›å›¾ã€å¤æ‚åº¦/åœºæ™¯æŸ±çŠ¶ï¼ˆæŒ‰å¯ç”¨æ€§ç»˜åˆ¶ï¼‰
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    axes = axes.ravel()

    plotted = False
    if {'culture', 'model_standardized', 'response_time'} <= set(df.columns):
        pivot_rt = df.pivot_table(values='response_time', index='culture', columns='model_standardized', aggfunc='mean')
        if pivot_rt.notna().any().any():
            sns.heatmap(pivot_rt, annot=True, cmap='YlOrRd', ax=axes[0], cbar_kws={'label':'å¹³å‡å“åº”æ—¶é—´(ç§’)'})
            axes[0].set_title('æ–‡åŒ–Ã—æ¨¡å‹ï¼šå“åº”æ—¶é—´'); plotted = True
    if {'culture', 'model_standardized', 'word_count'} <= set(df.columns):
        pivot_wc = df.pivot_table(values='word_count', index='culture', columns='model_standardized', aggfunc='mean')
        if pivot_wc.notna().any().any():
            sns.heatmap(pivot_wc, annot=True, cmap='Blues', ax=axes[1], cbar_kws={'label':'å¹³å‡è¯æ•°'})
            axes[1].set_title('æ–‡åŒ–Ã—æ¨¡å‹ï¼šè¯æ•°'); plotted = True

    if {'culture','complexity','response_time'} <= set(df.columns) and df['response_time'].notna().any():
        bar = df.groupby(['culture','complexity'])['response_time'].mean().unstack()
        bar.plot(kind='bar', ax=axes[2], color=['lightcoral','lightblue'])
        axes[2].set_title('ä¸åŒå¤æ‚åº¦çš„å“åº”æ—¶é—´'); axes[2].set_ylabel('ç§’'); axes[2].tick_params(axis='x', rotation=20); plotted = True
    else:
        axes[2].set_visible(False)

    if {'culture','scenario','word_count'} <= set(df.columns) and df['word_count'].notna().any():
        bar2 = df.groupby(['culture','scenario'])['word_count'].mean().unstack()
        bar2.plot(kind='bar', ax=axes[3], colormap='tab20')
        axes[3].set_title('ä¸åŒåœºæ™¯çš„è¯æ•°'); axes[3].set_ylabel('è¯æ•°'); axes[3].tick_params(axis='x', rotation=20); plotted = True
    else:
        axes[3].set_visible(False)

    plt.tight_layout()
    plt.savefig(os.path.join(fig_dir, "cultural_model_analysis.png"), dpi=300, bbox_inches='tight')
    plt.close()
    print("ä¿å­˜ï¼šfigures/cultural_model_analysis.png")

    return num_cols


# ---------- äº¤äº’å¼ä»ªè¡¨æ¿ï¼ˆå¯é€‰ï¼‰ ----------
def export_dashboard(df, out_dir):
    if not PLOTLY_AVAILABLE:
        print("æœªå®‰è£… plotlyï¼Œè·³è¿‡äº¤äº’å¼ä»ªè¡¨æ¿ã€‚")
        return
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('æ–‡åŒ–å“åº”æ—¶é—´', 'æ¨¡å‹å“åº”æ—¶é—´', 'å¤æ‚åº¦å“åº”æ—¶é—´', 'åœºæ™¯è¯æ•°'),
        specs=[[{"type": "bar"}, {"type": "bar"}],
               [{"type": "bar"}, {"type": "bar"}]]
    )
    if 'culture' in df.columns and 'response_time' in df.columns:
        m = df.groupby('culture')['response_time'].mean()
        fig.add_bar(x=m.index, y=m.values, name='æ–‡åŒ–-å“åº”æ—¶é—´', row=1, col=1)
    if 'model_standardized' in df.columns and 'response_time' in df.columns:
        m = df.groupby('model_standardized')['response_time'].mean()
        fig.add_bar(x=m.index, y=m.values, name='æ¨¡å‹-å“åº”æ—¶é—´', row=1, col=2)
    if 'complexity' in df.columns and 'response_time' in df.columns:
        m = df.groupby('complexity')['response_time'].mean()
        fig.add_bar(x=m.index, y=m.values, name='å¤æ‚åº¦-å“åº”æ—¶é—´', row=2, col=1)
    if 'scenario' in df.columns and 'word_count' in df.columns:
        m = df.groupby('scenario')['word_count'].mean()
        fig.add_bar(x=m.index, y=m.values, name='åœºæ™¯-è¯æ•°', row=2, col=2)

    fig.update_layout(height=800, showlegend=False, title_text="è·¨æ–‡åŒ–AIè¯„ä¼°äº¤äº’å¼ä»ªè¡¨æ¿", title_x=0.5)
    html_path = os.path.join(out_dir, "interactive_dashboard.html")
    fig.write_html(html_path)
    print(f"ä¿å­˜ï¼š{html_path}")


# ---------- ç®€çŸ­ Markdown æŠ¥å‘Š ----------
def export_report(df, out_dir):
    def uniq(col):
        return ', '.join([str(x) for x in df[col].dropna().unique().tolist()]) if col in df.columns else 'NA'

    rpt = []
    rpt.append("# è·¨æ–‡åŒ–AIè¯„ä¼°æ•°æ®åˆ†ææŠ¥å‘Š\n")
    rpt.append("## æ•°æ®æ¦‚è§ˆ")
    rpt.append(f"- æ€»æ ·æœ¬æ•°: {len(df):,}")
    rpt.append(f"- æ–‡åŒ–èƒŒæ™¯: {uniq('culture')}")
    rpt.append(f"- AIæ¨¡å‹: {uniq('model_standardized') if 'model_standardized' in df.columns else uniq('model')}")
    if 'complexity' in df.columns: rpt.append(f"- å¤æ‚åº¦: {uniq('complexity')}")
    if 'language' in df.columns: rpt.append(f"- è¯­è¨€: {uniq('language')}")
    if 'scenario' in df.columns: rpt.append(f"- åœºæ™¯: {uniq('scenario')}")

    if 'response_time' in df.columns and df['response_time'].notna().any():
        rpt.append("\n## å“åº”æ—¶é—´")
        rpt.append(f"- å¹³å‡: {df['response_time'].mean():.2f} ç§’")
        rpt.append(f"- æ ‡å‡†å·®: {df['response_time'].std():.2f} ç§’")
        rpt.append(f"- èŒƒå›´: {df['response_time'].min():.2f} ~ {df['response_time'].max():.2f} ç§’")

    if 'word_count' in df.columns and df['word_count'].notna().any():
        rpt.append("\n## è¯æ•°")
        rpt.append(f"- å¹³å‡: {df['word_count'].mean():.1f} è¯")
        rpt.append(f"- æ ‡å‡†å·®: {df['word_count'].std():.1f} è¯")
        rpt.append(f"- èŒƒå›´: {df['word_count'].min():.0f} ~ {df['word_count'].max():.0f} è¯")

    if {'culture','response_time'} <= set(df.columns):
        m = df.groupby('culture')['response_time'].agg(['mean','std','count'])
        rpt.append("\n## æ–‡åŒ–å·®å¼‚ï¼ˆå“åº”æ—¶é—´ï¼Œå‡å€¼Â±SD, nï¼‰")
        for idx, row in m.iterrows():
            rpt.append(f"- {idx}: {row['mean']:.2f}Â±{row['std']:.2f} ç§’ (n={int(row['count'])})")

    if {'model_standardized','response_time'} <= set(df.columns):
        m = df.groupby('model_standardized')['response_time'].agg(['mean','std','count'])
        rpt.append("\n## æ¨¡å‹å·®å¼‚ï¼ˆå“åº”æ—¶é—´ï¼Œå‡å€¼Â±SD, nï¼‰")
        for idx, row in m.iterrows():
            rpt.append(f"- {idx}: {row['mean']:.2f}Â±{row['std']:.2f} ç§’ (n={int(row['count'])})")

    rpt.append("\n---")
    rpt.append(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")

    os.makedirs(out_dir, exist_ok=True)
    path = os.path.join(out_dir, "analysis_report.md")
    with open(path, "w", encoding="utf-8") as f:
        f.write("\n".join(rpt))
    print(f"ä¿å­˜ï¼š{path}")


# ---------- ä¸»æµç¨‹ ----------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--raw-dir", default="data/raw", help="JSON åŸå§‹ç›®å½•")
    parser.add_argument("--out", default="reports", help="è¾“å‡ºç›®å½•ï¼ˆå›¾ã€è¡¨ã€æŠ¥å‘Šï¼‰")
    parser.add_argument("--csv", default="data/processed/merged_cross_cultural_data.csv", help="åˆå¹¶åçš„CSVè¾“å‡ºè·¯å¾„")
    parser.add_argument("--with-plotly", action="store_true", help="ç”Ÿæˆäº¤äº’å¼HTMLï¼ˆéœ€å®‰è£… plotlyï¼‰")
    args = parser.parse_args()

    # 1) åˆå¹¶
    merged = merge_raw_json(args.raw_dir)
    if merged.empty:
        print("æ²¡æœ‰å¯ç”¨æ•°æ®ï¼Œé€€å‡ºã€‚"); return

    # 2) é¢„å¤„ç†
    merged = preprocess(merged)

    # 3) ä¿å­˜åˆå¹¶CSV
    os.makedirs(os.path.dirname(args.csv), exist_ok=True)
    merged.to_csv(args.csv, index=False, encoding="utf-8")
    print(f"ğŸ’¾ åˆå¹¶æ•°æ®å·²ä¿å­˜ï¼š{args.csv}  å½¢çŠ¶={merged.shape}")

    # 4) å‡ºè¡¨+å‡ºå›¾
    os.makedirs(args.out, exist_ok=True)
    export_tables_and_figs(merged, args.out)

    # 5) äº¤äº’å¼ä»ªè¡¨æ¿ï¼ˆå¯é€‰ï¼‰
    if args.with-plotly:
        export_dashboard(merged, args.out)

    # 6) æŠ¥å‘Š
    export_report(merged, args.out)

    # 7) ç»ˆç«¯æ€»ç»“
    fastest_culture = merged.groupby('culture')['response_time'].mean().dropna().sort_values().index[0] \
                        if {'culture','response_time'} <= set(merged.columns) and merged['response_time'].notna().any() else 'NA'
    fastest_model = merged.groupby('model_standardized')['response_time'].mean().dropna().sort_values().index[0] \
                        if {'model_standardized','response_time'} <= set(merged.columns) and merged['response_time'].notna().any() else 'NA'
    print("\n====== æ€»ç»“ ======")
    print(f"- æ ·æœ¬æ•°ï¼š{len(merged):,}")
    print(f"- æ–‡åŒ–æ•°ï¼š{merged['culture'].nunique() if 'culture' in merged.columns else 'NA'}")
    print(f"- æ¨¡å‹æ•°ï¼š{merged['model_standardized'].nunique() if 'model_standardized' in merged.columns else 'NA'}")
    print(f"- å“åº”æœ€å¿«æ–‡åŒ–ï¼š{fastest_culture}")
    print(f"- å“åº”æœ€å¿«æ¨¡å‹ï¼š{fastest_model}")
    print("å®Œæˆ reports/")

if __name__ == "__main__":
    main()
